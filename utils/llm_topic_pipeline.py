"""
Content Engagement Prediction Agent
Context-aware content scoring system for comparative analysis

ğŸ¯ ROLE:
- Generate marketing and social content
- Optimize existing content
- Produce meaningful variants
- Evaluate variants using RELATIVE engagement scoring (0-100)

ğŸš¨ CRITICAL: DOES NOT PREDICT REAL-WORLD METRICS
- Outputs are RELATIVE SCORES for comparison, not real likes/comments/upvotes
- Term used: "Engagement Score" or "Relative Engagement Index"
- Ranking matters, absolute values do not

ğŸ“ SCORING RULES (MANDATORY):
- Use relative scoring (0-100 integers)
- Compare variants against each other
- Consider: hook strength, emotional resonance, CTA clarity, readability, platform alignment
- Rating buckets: Low (0-30), Average (31-50), Good (51-70), High (71-85), Viral Potential (86-100)
- Confidence levels: Low/Medium/High (never N/A)

ğŸ§  TRANSPARENCY REQUIREMENT:
"This system does not predict real engagement counts. It uses a relative engagement 
scoring framework to compare content effectiveness."

ğŸ—ï¸ TECHNICAL ARCHITECTURE:
- Google Gemini API with contextual learning (NO weight-level training)
- Topic-level statistical signals injected as context
- Feature-driven scoring + LLM reasoning
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import os
import pickle
import json
import time
import re
from textblob import TextBlob
import warnings
warnings.filterwarnings('ignore')

try:
    import streamlit as st
    import google.generativeai as genai
    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False

def get_api_key():
    """Get API key from multiple sources (Streamlit secrets, env, or toml file)"""
    # Try Streamlit secrets first (when running in Streamlit)
    try:
        import streamlit as st
        api_key = st.secrets.get("google_api_key")
        if api_key:
            return api_key
    except:
        pass
    
    # Try environment variable
    api_key = os.environ.get("GOOGLE_API_KEY")
    if api_key:
        return api_key
    
    # Try reading secrets.toml directly (when running standalone)
    try:
        import tomli
        secrets_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), '.streamlit', 'secrets.toml')
        if os.path.exists(secrets_path):
            with open(secrets_path, 'rb') as f:
                secrets = tomli.load(f)
                return secrets.get('google_api_key')
    except:
        pass
    
    # Try toml library as fallback
    try:
        import toml
        secrets_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), '.streamlit', 'secrets.toml')
        if os.path.exists(secrets_path):
            secrets = toml.load(secrets_path)
            return secrets.get('google_api_key')
    except:
        pass
    
    return None

try:
    from .dynamic_collector import DynamicTopicCollector
except:
    from dynamic_collector import DynamicTopicCollector


class LLMTopicPipeline:
    """
    Complete topic-driven pipeline using LLM approach (your original architecture)
    Collects topic data â†’ Computes statistics â†’ Feeds to Google Gemini as context
    """
    
    def __init__(self, topic: str, model='gemini-2.5-flash'):
        """
        Initialize LLM-based pipeline
        
        Args:
            topic: User's topic (e.g., "Milton bottles", "Nike shoes")
            model: Gemini model to use (default: gemini-2.5-flash - Paid Tier 1, fast and cost-effective)
        """
        self.topic = topic
        self.topic_slug = topic.lower().replace(' ', '_').replace('/', '_')
        self.model = model
        self._gemini_available = None
        self._rate_limit_delay = 0.5  # Minimal safety pause for Paid Tier 1 (1500 RPM limit)
        self._last_request_time = 0
        
        # Data storage
        self.raw_data = None
        self.processed_data = None
        self.historical_stats = None
        
        # Paths
        self.data_dir = 'data/topics'
        self.stats_dir = 'data/stats'
        os.makedirs(self.data_dir, exist_ok=True)
        os.makedirs(self.stats_dir, exist_ok=True)
        
        # Initialize Gemini
        if GEMINI_AVAILABLE:
            try:
                api_key = get_api_key()
                if api_key:
                    genai.configure(api_key=api_key)
                    
                    # Verify model is available
                    try:
                        available_models = [m.name for m in genai.list_models() if 'generateContent' in m.supported_generation_methods]
                        if self.model not in available_models and f'models/{self.model}' not in available_models:
                            print(f"âš ï¸ Model '{self.model}' not found. Available flash models:")
                            flash_models = [m for m in available_models if 'flash' in m.lower()]
                            for m in flash_models[:5]:
                                print(f"   - {m}")
                    except Exception:
                        pass  # Skip diagnostic if it fails
                    
                    self._gemini_available = True
                    print(f"âœ… Gemini API configured with model: {self.model}")
                else:
                    self._gemini_available = False
            except Exception as e:
                print(f"âš ï¸ Error configuring Gemini: {e}")
                self._gemini_available = False
        
        print(f"ğŸ¯ Initialized LLM pipeline for: '{topic}'")
    
    def _check_gemini(self):
        """Check if Gemini API is available"""
        if not GEMINI_AVAILABLE:
            print("âš ï¸ google-generativeai library not installed")
            return False
        
        if self._gemini_available is None:
            try:
                api_key = get_api_key()
                if api_key:
                    genai.configure(api_key=api_key)
                    self._gemini_available = True
                    print(f"âœ… Gemini API configured with {self.model}")
                else:
                    print("âš ï¸ google_api_key not found in secrets.toml")
                    self._gemini_available = False
            except Exception as e:
                print(f"âš ï¸ Error checking Gemini: {e}")
                self._gemini_available = False
        
        return self._gemini_available
    
    # ==================== STEP 1: DATA COLLECTION ====================
    
    def collect_data(self, reddit_limit=200, youtube_limit=0, force_refresh=False):
        """
        Collect topic-specific data from Reddit/YouTube
        
        Returns:
            dict: Collection summary
        """
        print(f"\n{'='*60}")
        print(f"STEP 1: COLLECTING DATA FOR '{self.topic}'")
        print(f"{'='*60}")
        
        # Check cache
        cache_file = f"{self.data_dir}/{self.topic_slug}_raw.pkl"
        
        if not force_refresh and os.path.exists(cache_file):
            print(f"ğŸ“¦ Loading cached data...")
            with open(cache_file, 'rb') as f:
                self.raw_data = pickle.load(f)
            print(f"âœ… Loaded {self.raw_data['total_posts']} posts from cache")
            return self.raw_data
        
        # Collect fresh data
        print(f"ğŸ“¡ Collecting fresh data...")
        collector = DynamicTopicCollector()
        
        self.raw_data = collector.collect_for_topic(
            topic=self.topic,
            reddit_limit=reddit_limit,
            youtube_limit=youtube_limit,
            save_local=False  # Direct upload: Collector â†’ Google Sheets â†’ Supabase
        )
        
        # Save to cache
        with open(cache_file, 'wb') as f:
            pickle.dump(self.raw_data, f)
        
        print(f"âœ… Collected {self.raw_data['total_posts']} posts")
        print(f"ğŸ’¾ Cached to {cache_file}")
        
        return self.raw_data
    
    # ==================== STEP 2: DATA PROCESSING ====================
    
    def process_data(self):
        """
        Process collected data and extract features
        
        Returns:
            DataFrame: Processed data
        """
        print(f"\n{'='*60}")
        print(f"STEP 2: PROCESSING DATA")
        print(f"{'='*60}")
        
        if self.raw_data is None or self.raw_data['combined_df'] is None:
            print("âŒ No data collected yet. Run collect_data() first.")
            return None
        
        df = self.raw_data['combined_df'].copy()
        
        print(f"Processing {len(df)} posts...")
        
        # Ensure 'text' column exists (use 'selftext' if available, otherwise use 'title')
        if 'text' not in df.columns:
            if 'selftext' in df.columns:
                df['text'] = df['selftext'].fillna('')
            elif 'title' in df.columns:
                df['text'] = df['title'].fillna('')
            else:
                df['text'] = ''
        
        # Add sentiment analysis
        print("   ğŸ” Analyzing sentiment...")
        df['sentiment_polarity'] = df['text'].apply(
            lambda x: TextBlob(str(x)).sentiment.polarity if pd.notna(x) else 0
        )
        df['sentiment_subjectivity'] = df['text'].apply(
            lambda x: TextBlob(str(x)).sentiment.subjectivity if pd.notna(x) else 0
        )
        
        # Add text features
        df['text_length'] = df['text'].str.len().fillna(0)
        df['word_count'] = df['text'].str.split().str.len().fillna(0)
        df['has_hashtag'] = df['text'].str.contains('#', na=False)
        df['has_url'] = df['text'].str.contains('http', na=False)
        df['has_emoji'] = df['text'].apply(self._has_emoji)
        
        # Add timing features
        df['created_at'] = pd.to_datetime(df['created_at'], errors='coerce', utc=True)
        df['hour_of_day'] = df['created_at'].dt.hour
        df['day_of_week'] = df['created_at'].dt.dayofweek
        df['day_name'] = df['created_at'].dt.day_name()
        
        # Calculate engagement
        if 'engagement' not in df.columns:
            # For Reddit: score + comments
            if 'score' in df.columns:
                df['score'] = pd.to_numeric(df['score'], errors='coerce').fillna(0)
                df['num_comments'] = pd.to_numeric(df['num_comments'], errors='coerce').fillna(0)
                df['engagement'] = df['score'] + df['num_comments']
            # For Twitter: likes + retweets + replies
            elif 'likes' in df.columns:
                df['likes'] = pd.to_numeric(df['likes'], errors='coerce').fillna(0)
                df['retweets'] = pd.to_numeric(df['retweets'], errors='coerce').fillna(0)
                df['replies'] = pd.to_numeric(df['replies'], errors='coerce').fillna(0)
                df['engagement'] = df['likes'] + df['retweets'] + df['replies']
            else:
                df['engagement'] = 0
        
        self.processed_data = df
        
        # Save processed data
        processed_file = f"{self.data_dir}/{self.topic_slug}_processed.pkl"
        df.to_pickle(processed_file)
        
        print(f"âœ… Processed {len(df)} posts")
        print(f"ğŸ’¾ Saved to {processed_file}")
        
        return df
    
    def _has_emoji(self, text):
        """Check if text contains emojis"""
        if pd.isna(text):
            return False
        emoji_pattern = re.compile("["
            u"\U0001F600-\U0001F64F"  # emoticons
            u"\U0001F300-\U0001F5FF"  # symbols & pictographs
            u"\U0001F680-\U0001F6FF"  # transport & map symbols
            u"\U0001F1E0-\U0001F1FF"  # flags
            "]+", flags=re.UNICODE)
        return bool(emoji_pattern.search(text))
    
    def _extract_structural_features(self, text):
        """
        Extract structural features for independent variant analysis
        Each variant must be analyzed based on its OWN structure
        """
        features = {
            'length': len(text),
            'word_count': len(text.split()),
            'has_question': '?' in text,
            'question_count': text.count('?'),
            'has_cta': any(word in text.lower() for word in [
                'comment', 'share', 'reply', 'follow', 'subscribe', 
                'click', 'learn more', 'check out', 'try', 'get'
            ]),
            'emoji_count': sum(1 for c in text if ord(c) > 127000),
            'has_hashtags': '#' in text,
            'hashtag_count': text.count('#'),
            'has_url': 'http' in text or 'www.' in text,
            'uppercase_ratio': sum(1 for c in text if c.isupper()) / max(len(text), 1),
            'sentence_count': max(text.count('.'), text.count('!'), text.count('?'), 1)
        }
        return features
    
    # ==================== STEP 3: COMPUTE STATISTICS FOR LLM ====================
    
    def compute_statistics(self):
        """
        Compute historical statistics from collected data
        This is what feeds the LLM as context (your original approach)
        
        Returns:
            dict: Statistics for LLM context
        """
        print(f"\n{'='*60}")
        print(f"STEP 3: COMPUTING STATISTICS FOR LLM CONTEXT")
        print(f"{'='*60}")
        
        if self.processed_data is None:
            print("âŒ No processed data. Run process_data() first.")
            return None
        
        df = self.processed_data
        
        print(f"Computing stats from {len(df)} posts...")
        
        stats = {
            'topic': self.topic,
            'total_posts': len(df),
            'date_computed': datetime.now().isoformat(),
            
            # Overall engagement
            'engagement': {
                'average': float(df['engagement'].mean()),
                'median': float(df['engagement'].median()),
                'max': float(df['engagement'].max()),
                'min': float(df['engagement'].min()),
                'std': float(df['engagement'].std()),
                'top_25_percent': float(df['engagement'].quantile(0.75)),
                'top_10_percent': float(df['engagement'].quantile(0.90))
            },
            
            # Sentiment patterns
            'sentiment': {
                'average_polarity': float(df['sentiment_polarity'].mean()),
                'positive_posts': int((df['sentiment_polarity'] > 0.1).sum()),
                'negative_posts': int((df['sentiment_polarity'] < -0.1).sum()),
                'neutral_posts': int(((df['sentiment_polarity'] >= -0.1) & (df['sentiment_polarity'] <= 0.1)).sum())
            },
            
            # Timing patterns
            'timing': {
                'best_hours': df.groupby('hour_of_day')['engagement'].mean().nlargest(5).to_dict(),
                'best_days': df.groupby('day_name')['engagement'].mean().nlargest(3).to_dict(),
                'worst_hours': df.groupby('hour_of_day')['engagement'].mean().nsmallest(3).to_dict()
            },
            
            # Content patterns
            'content': {
                'avg_length': float(df['text_length'].mean()),
                'avg_words': float(df['word_count'].mean()),
                'hashtag_usage': float((df['has_hashtag']).mean() * 100),
                'url_usage': float((df['has_url']).mean() * 100),
                'emoji_usage': float((df['has_emoji']).mean() * 100)
            },
            
            # Impact analysis
            'impact': {
                'hashtag_boost': self._compute_feature_impact(df, 'has_hashtag'),
                'url_impact': self._compute_feature_impact(df, 'has_url'),
                'emoji_impact': self._compute_feature_impact(df, 'has_emoji')
            },
            
            # Top performing examples
            'top_posts': df.nlargest(10, 'engagement')[['text', 'engagement', 'sentiment_polarity']].to_dict('records')
        }
        
        self.historical_stats = stats
        
        # Save stats
        stats_file = f"{self.stats_dir}/{self.topic_slug}_stats.json"
        with open(stats_file, 'w') as f:
            json.dump(stats, f, indent=2)
        
        print(f"âœ… Computed statistics:")
        print(f"   - Average engagement: {stats['engagement']['average']:.1f}")
        print(f"   - Top 10% threshold: {stats['engagement']['top_10_percent']:.1f}")
        print(f"   - Average sentiment: {stats['sentiment']['average_polarity']:.3f}")
        print(f"   - Best posting hour: {max(stats['timing']['best_hours'], key=stats['timing']['best_hours'].get)}")
        print(f"ğŸ’¾ Saved to {stats_file}")
        
        return stats
    
    def _compute_feature_impact(self, df, feature_col):
        """Compute impact of a feature on engagement"""
        with_feature = df[df[feature_col]]['engagement'].mean()
        without_feature = df[~df[feature_col]]['engagement'].mean()
        
        if without_feature > 0:
            improvement = ((with_feature - without_feature) / without_feature) * 100
        else:
            improvement = 0
        
        return {
            'with': float(with_feature),
            'without': float(without_feature),
            'improvement_pct': float(improvement)
        }
    
    # ==================== STEP 4: GENERATE LLM CONTEXT ====================
    
    def generate_llm_context(self):
        """
        Generate formatted context string for LLM
        This is fed to Llama 3.2 before every generation/prediction
        
        Returns:
            str: Formatted context
        """
        if self.historical_stats is None:
            print("âš ï¸ No statistics computed. Run compute_statistics() first.")
            return ""
        
        stats = self.historical_stats
        
        context = f"""You are an expert social media analyst specializing in '{self.topic}'.

TOPIC ANALYSIS ({stats['total_posts']} posts analyzed):

ENGAGEMENT PATTERNS:
- Average Engagement: {stats['engagement']['average']:.0f}
- Top Performers: {stats['engagement']['top_10_percent']:.0f}+ engagement
- Engagement Range: {stats['engagement']['min']:.0f} to {stats['engagement']['max']:.0f}

SENTIMENT INSIGHTS:
- Overall Sentiment: {'Positive' if stats['sentiment']['average_polarity'] > 0.1 else 'Neutral' if stats['sentiment']['average_polarity'] > -0.1 else 'Negative'}
- Positive Posts: {stats['sentiment']['positive_posts']} ({stats['sentiment']['positive_posts']/stats['total_posts']*100:.1f}%)
- Negative Posts: {stats['sentiment']['negative_posts']} ({stats['sentiment']['negative_posts']/stats['total_posts']*100:.1f}%)

OPTIMAL POSTING TIMES:
- Best Hours: {', '.join([f"{int(h)}:00 ({e:.0f} avg)" for h, e in list(stats['timing']['best_hours'].items())[:3]])}
- Best Days: {', '.join([f"{d} ({e:.0f} avg)" for d, e in list(stats['timing']['best_days'].items())[:3]])}

CONTENT BEST PRACTICES:
- Ideal Length: {stats['content']['avg_length']:.0f} characters
- Ideal Word Count: {stats['content']['avg_words']:.0f} words
- Hashtag Usage: {stats['content']['hashtag_usage']:.0f}% (Impact: {stats['impact']['hashtag_boost']['improvement_pct']:+.1f}%)
- URL Inclusion: {stats['content']['url_usage']:.0f}% (Impact: {stats['impact']['url_impact']['improvement_pct']:+.1f}%)
- Emoji Usage: {stats['content']['emoji_usage']:.0f}% (Impact: {stats['impact']['emoji_impact']['improvement_pct']:+.1f}%)

TOP PERFORMING CONTENT EXAMPLES:
"""
        
        for i, post in enumerate(stats['top_posts'][:5], 1):
            context += f"{i}. \"{post['text'][:100]}...\" (Engagement: {post['engagement']:.0f}, Sentiment: {post['sentiment_polarity']:.2f})\n"
        
        context += f"\nUse these insights to generate content and predict engagement for '{self.topic}'."
        
        return context
    
    # ==================== STEP 5: LLM GENERATION ====================
    
    def generate_content(self, prompt, max_tokens=2048):
        """
        Generate content using Gemini with topic context
        
        Args:
            prompt: User prompt (e.g., "Write a tweet about Milton bottles")
            max_tokens: Max output tokens (default: 2048 for Paid Tier)
            
        Returns:
            str: Generated content
        """
        if not self._check_gemini():
            return "âŒ Gemini API not configured. Add google_api_key to secrets.toml"
        
        # Minimal safety pause for Paid Tier 1 (optional)
        current_time = time.time()
        time_since_last = current_time - self._last_request_time
        if time_since_last < self._rate_limit_delay:
            sleep_time = self._rate_limit_delay - time_since_last
            time.sleep(sleep_time)
        
        # Build full prompt with context
        context = self.generate_llm_context()
        
        # Adjust prompt for current tier limitations
        # If running on free tier (during Paid Tier activation), request concise content
        focused_prompt = f"""{context}

USER REQUEST: {prompt}

IMPORTANT: Generate a complete, concise response in 150-200 words. Be direct and engaging.
Focus on delivering a finished piece, not an introduction to a longer response.

Your response:"""
        
        print(f"\nğŸ¤– Generating content with Gemini...")
        
        try:
            model = genai.GenerativeModel(self.model)
            
            # Retry logic with exponential backoff for Paid Tier 1
            max_retries = 3
            retry_count = 0
            base_wait_time = 2  # Start with 2 seconds
            
            while retry_count < max_retries:
                try:
                    response = model.generate_content(
                        focused_prompt,
                        generation_config={
                            'temperature': 0.9,
                            'max_output_tokens': max_tokens,
                            'top_p': 0.95,
                            'top_k': 40,
                            'candidate_count': 1
                        },
                        safety_settings={
                            'HARM_CATEGORY_HARASSMENT': 'BLOCK_NONE',
                            'HARM_CATEGORY_HATE_SPEECH': 'BLOCK_NONE',
                            'HARM_CATEGORY_SEXUALLY_EXPLICIT': 'BLOCK_NONE',
                            'HARM_CATEGORY_DANGEROUS_CONTENT': 'BLOCK_NONE',
                        }
                    )
                    
                    self._last_request_time = time.time()
                    
                    # Check if response was blocked or incomplete
                    if not response.candidates:
                        return "âŒ Response blocked by safety filters"
                    
                    candidate = response.candidates[0]
                    
                    # Check finish reason
                    if hasattr(candidate, 'finish_reason'):
                        finish_reason = int(candidate.finish_reason)
                        if finish_reason == 3:  # SAFETY
                            return "âŒ Response blocked by safety filters"
                        elif finish_reason == 2:  # MAX_TOKENS - Try to continue
                            generated = response.text.strip()
                            print(f"âš ï¸ Response truncated at {len(generated)} chars. Attempting continuation...")
                            
                            # Try to get continuation (if Paid Tier allows)
                            try:
                                continuation_prompt = f"{focused_prompt}\n\nPREVIOUS PARTIAL RESPONSE:\n{generated}\n\nPlease complete the response from where it was cut off:"
                                continuation_response = model.generate_content(
                                    continuation_prompt,
                                    generation_config={
                                        'temperature': 0.9,
                                        'max_output_tokens': max_tokens,
                                        'top_p': 0.95,
                                        'top_k': 40,
                                    }
                                )
                                
                                if continuation_response.candidates and int(continuation_response.candidates[0].finish_reason) == 1:
                                    # Got complete continuation
                                    full_text = generated + " " + continuation_response.text.strip()
                                    print(f"âœ… Continuation successful! Total: {len(full_text)} characters")
                                    return full_text
                                else:
                                    # Continuation also truncated, return what we have with warning
                                    print(f"âš ï¸ Continuation also truncated. Paid Tier may not be active.")
                                    return generated + "\n\nâš ï¸ [Note: Response incomplete. Your Paid Tier may take 2-24 hours to activate. Check: https://ai.dev/usage]"
                            except:
                                # Continuation failed, return partial with warning
                                return generated + "\n\nâš ï¸ [Note: Response incomplete. Your Paid Tier may take 2-24 hours to activate. Check: https://ai.dev/usage]"
                    
                    generated = response.text.strip()
                    print(f"âœ… Generated {len(generated)} characters (Complete)")
                    return generated
                    
                except Exception as api_error:
                    error_msg = str(api_error)
                    if '429' in error_msg or 'Resource' in error_msg:
                        retry_count += 1
                        if retry_count < max_retries:
                            # Exponential backoff: 2s, 4s, 8s
                            wait_time = base_wait_time * (2 ** (retry_count - 1))
                            print(f"âš ï¸ Rate limit hit. Retry {retry_count}/{max_retries} after {wait_time}s...")
                            time.sleep(wait_time)
                            continue
                        else:
                            print(f"âŒ Max retries reached after rate limit errors")
                            return "âŒ Rate limit exceeded after retries"
                    raise
            
            return "âŒ Unexpected error: max retries reached"
                
        except Exception as e:
            return f"âŒ Generation failed: {e}"
    
    # ==================== STEP 6: LLM PREDICTION ====================
    
    def predict_engagement(self, content):
        """
        Predict engagement using Gemini with topic context
        STATELESS: Each call produces independent analysis
        
        Args:
            content: Text to predict engagement for
            
        Returns:
            dict: Prediction results with unique analysis
        """
        if not self._check_gemini():
            return {"error": "Gemini API not configured"}
        
        # Minimal safety pause for Paid Tier 1
        current_time = time.time()
        time_since_last = current_time - self._last_request_time
        if time_since_last < self._rate_limit_delay:
            sleep_time = self._rate_limit_delay - time_since_last
            time.sleep(sleep_time)
        
        # Extract structural features (for independent analysis)
        features = self._extract_structural_features(content)
        
        # Build prediction prompt with comprehensive framework
        context = self.generate_llm_context()
        
        # Build prompt following absolute rules (NO raw numbers exposed)
        # Include structural features for independent analysis
        prompt = f"""{context}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
CONTENT ENGAGEMENT PREDICTION AGENT
Platform-Aware â€¢ Realistic â€¢ Statistically Honest
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

TOPIC: '{self.topic}'
PLATFORMInferred from content style

CONTENT TO ANALYZE:
"{content}"

ğŸ” STRUCTURAL ANALYSIS (MANDATORY CONSIDERATION):
- Length: {features['length']} characters ({features['word_count']} words)
- Questions: {'Yes' if features['has_question'] else 'No'} ({features['question_count']} question marks)
- Call-to-Action: {'Present' if features['has_cta'] else 'Missing'}
- Emojis: {features['emoji_count']}
- Hashtags: {features['hashtag_count']}
- URL: {'Yes' if features['has_url'] else 'No'}
- Sentences: {features['sentence_count']}

âš ï¸ YOU MUST ANALYZE THE ABOVE STRUCTURAL FEATURES INDEPENDENTLY.
Different structures = Different scores (even for similar topics).

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ¯ YOUR MISSION
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Produce REALISTIC, platform-aware, statistically honest evaluations.
Your goal is TRUST and CONSISTENCY â€” NOT impressive numbers.

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
1ï¸âƒ£ PLATFORM-AWARE LOGIC (MANDATORY)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Detect platform from content style:
- Reddit: Discussion posts, questions, long-form, conversational
- Twitter/X: Short, concise, hashtags common
- LinkedIn: Professional tone, career-focused

Platform-Specific Rules:
â€¢ Reddit:
  - Questions ARE valid CTAs (do NOT penalize)
  - Long-form conversational posts are ACCEPTABLE
  - DO NOT recommend emojis or hashtags by default
  - Prioritize: clarity, specificity, discussion-driving questions
  
â€¢ Twitter/X:
  - Emojis and hashtags may be recommended SPARINGLY
  - Brevity is valued (under 220 chars)
  
â€¢ LinkedIn:
  - Professional tone required
  - Emojis optional

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
2ï¸âƒ£ ENGAGEMENT SCORE CALIBRATION
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

CRITICAL: All scores MUST be 0â€“100 NORMALIZED integers.

Scale Mapping:
- 0â€“30   â†’ Low (weak hooks, poor platform fit)
- 31â€“50  â†’ Average (acceptable but unremarkable)
- 51â€“70  â†’ Good (strong content, clear engagement triggers)
- 71â€“85  â†’ High (excellent hooks, perfect platform fit)
- 86â€“100 â†’ Viral Potential (exceptional, rare)

NEVER output:
âŒ Raw numbers (366, 493, 1200, 500+)
âŒ Scores above 100
âŒ Non-integers or ranges

If historical data suggests 500 likes:
âœ… Normalize to 65-75 (above average)
âŒ DO NOT expose raw value

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
3ï¸âƒ£ REALISTIC QUALITY SCORING
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

If content is:
- Readable
- Matches platform tone
- Contains question or engagement trigger
â†’ MINIMUM score = 50-60 (Good)

Reserve LOW scores (0-30) ONLY for:
- Spam or unclear content
- Severe platform mismatch
- No engagement triggers whatsoever

Well-structured Reddit discussion posts:
â†’ Typical range: 55-75
â†’ Reserve 85+ for truly exceptional content

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
4ï¸âƒ£ CONFIDENCE HONESTY
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Confidence Levels:
â€¢ High â†’ Strong platform fit, clear signals, obvious quality
â€¢ Medium â†’ Mixed signals, balanced strengths/weaknesses
â€¢ Low â†’ Weak signals, ambiguous quality, risky content

NEVER show "High confidence" for:
- Average content (scores 31-50)
- Content with major gaps
- Platform mismatches

Confidence MUST align with score:
âœ… Score 75, High confidence â†’ Valid
âœ… Score 45, Medium confidence â†’ Valid
âŒ Score 35, High confidence â†’ INVALID
âŒ Score 80, Low confidence â†’ INVALID

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
5ï¸âƒ£ REASONING REQUIREMENTS
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Provide EXACTLY 3 bullet points explaining the score.
Each MUST reference concrete factors:
- Hook quality
- Platform alignment
- Question/CTA presence
- Readability
- Specificity
- Length appropriateness

Example reasoning:
âœ… "Strong discussion question invites community engagement"
âœ… "Length appropriate for Reddit (150 words), good context"
âœ… "Lacks specific examples or data to strengthen credibility"

âŒ "This is good content"
âŒ "Could be better"
âŒ Generic observations

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
6ï¸âƒ£ IMPROVEMENT SUGGESTIONS
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Provide ONE specific, actionable suggestion.

If content is already well-optimized:
âœ… "Content is already well optimized; only marginal gains possible"
âœ… "Add 1-2 concrete examples to strengthen credibility"

DO NOT suggest if not needed:
âŒ "Add emojis" (for Reddit discussion posts)
âŒ "Add hashtags" (for discussion-based content)
âŒ "Make it more engaging" (too generic)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
7ï¸âƒ£ FORBIDDEN OUTPUT
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

NEVER output:
- Raw engagement numbers (366, 493, 1200)
- "N/A" for confidence
- Fallback explanations
- Internal errors
- Scores above 100 or below 0
- Non-integer scores

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
OUTPUT FORMAT (STRICT)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

{{
    "predicted_engagement": <integer 0-100>,
    "rating": "<Low|Average|Good|High|Viral Potential>",
    "confidence": "<Low|Medium|High>",
    "reasoning": [
        "<Concrete reason 1>",
        "<Concrete reason 2>",
        "<Concrete reason 3>"
    ],
    "improvement_suggestion": "<One specific improvement>"
}}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
FINAL SELF-CHECK (SILENT)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Before responding, verify:
âœ… Score is 0-100 integer
âœ… Rating aligns with score range
âœ… Confidence aligns with score quality
âœ… Platform-appropriate reasoning
âœ… No forbidden outputs (no raw numbers!)
âœ… Realistic scoring (not exaggerated)

RESPOND ONLY WITH VALID JSON. NO OTHER TEXT."""
        
        print(f"\nğŸ“Š Predicting engagement with Gemini...")
        
        try:
            model = genai.GenerativeModel(self.model)
            
            # Retry logic for rate limits
            max_retries = 3
            retry_count = 0
            llm_response = None
            
            while retry_count < max_retries:
                try:
                    response = model.generate_content(
                        prompt,
                        generation_config={
                            'temperature': 0.3,
                            'max_output_tokens': 500
                        }
                    )
                    
                    self._last_request_time = time.time()
                    llm_response = response.text.strip()
                    break
                    
                except Exception as api_error:
                    error_msg = str(api_error)
                    if '429' in error_msg or 'Resource' in error_msg:
                        retry_count += 1
                        if retry_count < max_retries:
                            # Exponential backoff: 2s, 4s, 8s
                            base_wait_time = 2
                            wait_time = base_wait_time * (2 ** (retry_count - 1))
                            print(f"âš ï¸ Rate limit hit. Retry {retry_count}/{max_retries} after {wait_time}s...")
                            time.sleep(wait_time)
                            continue
                    raise
            
            if llm_response:
                # Try to parse JSON from response
                try:
                    # Extract JSON if wrapped in markdown
                    if '```json' in llm_response:
                        json_str = llm_response.split('```json')[1].split('```')[0].strip()
                    elif '```' in llm_response:
                        json_str = llm_response.split('```')[1].split('```')[0].strip()
                    else:
                        json_str = llm_response
                    
                    prediction = json.loads(json_str)
                    
                    # ===== STRICT VALIDATION & ENFORCEMENT =====
                    
                    # Rule 1 & 2: Normalize to 0-100 integer
                    if 'predicted_engagement' in prediction:
                        # Convert to integer (no decimals)
                        prediction['predicted_engagement'] = int(round(float(prediction['predicted_engagement'])))
                        # Clamp to 0-100 range
                        prediction['predicted_engagement'] = max(0, min(100, prediction['predicted_engagement']))
                    else:
                        prediction['predicted_engagement'] = 50  # Safe default
                    
                    score = prediction['predicted_engagement']
                    
                    # Rule 3: Confidence MUST be Low/Medium/High (never N/A)
                    valid_confidence = ['Low', 'Medium', 'High']
                    if prediction.get('confidence') not in valid_confidence:
                        prediction['confidence'] = 'Medium'
                    
                    # Rule 5: Rating MUST align with score (prevent contradictions)
                    rating_map = [
                        (0, 30, 'Low'),
                        (31, 50, 'Average'),
                        (51, 70, 'Good'),
                        (71, 85, 'High'),
                        (86, 100, 'Viral Potential')
                    ]
                    
                    correct_rating = None
                    for min_val, max_val, rating_label in rating_map:
                        if min_val <= score <= max_val:
                            correct_rating = rating_label
                            break
                    
                    if prediction.get('rating') != correct_rating:
                        prediction['rating'] = correct_rating
                    
                    # Rule 6: Ensure reasoning is list of 3 bullets
                    if 'reasoning' not in prediction or not isinstance(prediction['reasoning'], list):
                        prediction['reasoning'] = [
                            "Content structure aligns with platform best practices",
                            "Topic relevance matches audience expectations",
                            "Engagement drivers are moderately present"
                        ]
                    elif len(prediction['reasoning']) < 3:
                        # Pad if less than 3
                        while len(prediction['reasoning']) < 3:
                            prediction['reasoning'].append("Additional content factors considered")
                    elif len(prediction['reasoning']) > 3:
                        # Trim if more than 3
                        prediction['reasoning'] = prediction['reasoning'][:3]
                    
                    # Ensure improvement_suggestion exists
                    if 'improvement_suggestion' not in prediction or not prediction['improvement_suggestion']:
                        prediction['improvement_suggestion'] = "Add a stronger call-to-action to drive engagement"
                    
                    # Rule 4: Remove any forbidden phrases (silent cleanup)
                    forbidden_phrases = [
                        "unable to parse", "using historical average", 
                        "insufficient data", "fallback logic", "N/A"
                    ]
                    for field in ['reasoning', 'improvement_suggestion']:
                        if field in prediction:
                            if isinstance(prediction[field], list):
                                prediction[field] = [
                                    reason for reason in prediction[field]
                                    if not any(phrase.lower() in reason.lower() for phrase in forbidden_phrases)
                                ]
                            elif isinstance(prediction[field], str):
                                for phrase in forbidden_phrases:
                                    prediction[field] = prediction[field].replace(phrase, "")
                    
                    # Add benchmarks as integers
                    if self.historical_stats:
                        prediction['benchmark'] = {
                            'avg_engagement': int(round(self.historical_stats['engagement']['average'])),
                            'top_25_percent': int(round(self.historical_stats['engagement']['top_25_percent'])),
                            'top_10_percent': int(round(self.historical_stats['engagement']['top_10_percent']))
                        }
                    
                    print(f"âœ… Predicted: {prediction['predicted_engagement']} ({prediction['rating']}) - Confidence: {prediction['confidence']}")
                    return prediction
                    
                except json.JSONDecodeError as e:
                    # Fallback: Calculate score from CONTENT FEATURES (not topic average)
                    print(f"âš ï¸ JSON parse failed, using feature-based scoring")
                    
                    # Calculate score from structural features
                    base_score = 40  # Base for readable content
                    
                    # Add points for engagement features
                    if features['has_question']:
                        base_score += features['question_count'] * 5  # Questions drive engagement
                    if features['has_cta']:
                        base_score += 10  # CTAs increase interaction
                    if features['emoji_count'] > 0:
                        base_score += min(features['emoji_count'] * 3, 12)  # Emojis help (max +12)
                    if features['has_hashtags']:
                        base_score += min(features['hashtag_count'] * 4, 12)  # Hashtags (max +12)
                    if features['has_url']:
                        base_score -= 5  # URLs can reduce engagement on some platforms
                    
                    # Length penalty/bonus
                    if features['word_count'] < 10:
                        base_score -= 10  # Too short
                    elif features['word_count'] > 100:
                        base_score -= 5  # Too long
                    elif 20 <= features['word_count'] <= 50:
                        base_score += 5  # Optimal length
                    
                    # Uppercase penalty (shouting)
                    if features['uppercase_ratio'] > 0.3:
                        base_score -= 10
                    
                    # Clamp to 0-100
                    fallback_score = max(0, min(100, int(base_score)))
                    
                    # Determine rating from score
                    rating_map = [
                        (0, 30, 'Low'),
                        (31, 50, 'Average'),
                        (51, 70, 'Good'),
                        (71, 85, 'High'),
                        (86, 100, 'Viral Potential')
                    ]
                    fallback_rating = 'Average'
                    for min_val, max_val, rating_label in rating_map:
                        if min_val <= fallback_score <= max_val:
                            fallback_rating = rating_label
                            break
                    
                    return {
                        'predicted_engagement': fallback_score,
                        'rating': fallback_rating,
                        'confidence': 'Medium',
                        'reasoning': [
                            'Content length and structure are within acceptable range',
                            'Topic alignment with historical patterns is moderate',
                            'Engagement signals present but not strongly differentiated'
                        ],
                        'improvement_suggestion': 'Add a stronger opening hook to capture attention immediately'
                    }
            else:
                # No response fallback (UI-ready, no error exposure)
                return {
                    "predicted_engagement": 50,
                    "rating": "Average",
                    "confidence": "Medium",
                    "reasoning": [
                        "Standard content structure with balanced elements",
                        "Topic relevance aligns with general audience interest",
                        "Moderate engagement drivers present"
                    ],
                    "improvement_suggestion": "Incorporate specific data or examples to strengthen credibility"
                }
                
        except Exception as e:
            # Final fallback (UI-ready, no error exposure)
            return {
                "predicted_engagement": 50,
                "rating": "Average",
                "confidence": "Medium",
                "reasoning": [
                    "Content follows conventional structure patterns",
                    "Topic has baseline audience appeal",
                    "Engagement potential is moderately consistent"
                ],
                "improvement_suggestion": "Test different content angles to identify high-performing variations"
            }
    
    # ==================== COMPLETE PIPELINE ====================
    
    def run_full_pipeline(self, reddit_limit=200, youtube_limit=0):
        """
        Run complete pipeline: Collect â†’ Process â†’ Compute Stats
        
        Returns:
            dict: Pipeline results
        """
        print(f"\n{'â–ˆ'*60}")
        print(f"  RUNNING COMPLETE PIPELINE FOR '{self.topic}'")
        print(f"{'â–ˆ'*60}")
        
        try:
            # Step 1: Collect
            data = self.collect_data(reddit_limit, youtube_limit)
            
            # Step 2: Process
            processed = self.process_data()
            
            # Step 3: Compute stats
            stats = self.compute_statistics()
            
            print(f"\n{'='*60}")
            print(f"âœ… PIPELINE COMPLETE!")
            print(f"{'='*60}")
            print(f"ğŸ“Š Data collected: {data['total_posts']}")
            print(f"ğŸ“Š Data processed: {len(processed)}")
            print(f"ğŸ“Š Statistics computed: {stats['total_posts']} posts analyzed")
            print(f"\nğŸ¤– LLM ready for content generation and predictions!")
            
            return {
                'status': 'success',
                'data_collected': data['total_posts'],
                'data_processed': len(processed),
                'stats_computed': True,
                'avg_engagement': stats['engagement']['average']
            }
            
        except Exception as e:
            print(f"\nâŒ Pipeline failed: {e}")
            return {'status': 'failed', 'error': str(e)}
    
    # ==================== UTILITY METHODS ====================
    
    def get_analytics_data(self):
        """Get processed data for analytics"""
        return self.processed_data
    
    @staticmethod
    def list_available_topics():
        """List all topics from Supabase (single source of truth)"""
        try:
            from utils.supabase_db import get_supabase_client
            supabase = get_supabase_client()
            
            # Get unique topics from Reddit data
            query = supabase.client.table('reddit_data').select('topic').execute()
            
            if query.data:
                topics = set()
                for row in query.data:
                    if row.get('topic'):
                        topics.add(row['topic'])
                return sorted(list(topics))
            return []
        except Exception as e:
            print(f"âš ï¸ Error fetching topics from Supabase: {e}")
            # Fallback to local cache
            data_dir = 'data/topics'
            if not os.path.exists(data_dir):
                return []
            
            topics = set()
            for f in os.listdir(data_dir):
                if f.endswith('_raw.pkl'):
                    topic_slug = f.replace('_raw.pkl', '')
                    topic = topic_slug.replace('_', ' ').title()
                    topics.add(topic)
            
            return sorted(list(topics))
