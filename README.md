# ðŸš€ AI Content Marketing Platform - Multi-Page Web App

**Complete AI-powered marketing platform** with engagement predictions, content generation, A/B testing, campaign simulation, and comprehensive analytics.

**Status:** âœ… Production Ready - FULLY DEPLOYED WEB APP  
**Framework:** Streamlit Multi-Page Application (6 Pages)  
**Deployment:** 100% FREE on Streamlit Cloud  
**AI Model:** Ollama llama3.2 (Local, No API Costs)

---

## ðŸŽ¯ What This Is

A **professional 6-page web application** that helps marketers:
- Generate AI-powered content variants
- Predict engagement scores before posting
- Run A/B tests and track results
- Get AI coaching for content optimization
- Simulate multi-post campaigns
- Analyze historical performance
- Manage API connections

**100% FREE to deploy and use!**

---

## ðŸŒ Live Demo

Once deployed, your app will be at:
```
https://YOUR_USERNAME-YOUR_REPO.streamlit.app
```

**Local Testing:**
```bash
streamlit run Home.py
```
Visit: http://localhost:8501

---

## ðŸ“± Application Pages

### 1. ðŸ  Home Dashboard
- 30-day KPIs and metrics
- Best posting times analysis
- Engagement and sentiment trends
- Interactive Plotly charts
- Getting started guide

### 2. ðŸ”® Content Generator
- AI-powered variant generation (2-5 variants)
- 4 optimization actions:
  - Generate Variants
  - Optimize Tone
  - Add CTA
  - Add Hashtags
- Engagement predictions for each variant
- Winner selection with visualizations
- Slack notifications

### 3. ðŸ§ª A/B Testing Lab
- Manual entry OR CSV upload
- Support for 5 variants (A, B, C, D, E)
- Engagement predictions
- Winner announcement with gap analysis
- Auto-saves to Google Sheets
- 3 chart types: Pie, Bar, Scatter

### 4. ðŸ¤– Engagement Coach
- Full content analysis
- AI recommendations
- Quality score (5 indicators)
- Current vs potential gauge chart
- Optimal posting time suggestions
- Content breakdown (length, words, hashtags, emojis)

### 5. ðŸ“‹ Campaign Simulator
- Multi-post campaign planning (2-10 posts)
- 3 simulation modes:
  - **Optimal Timing** - Best time for each post
  - **Best Platform** - Twitter vs Reddit comparison
  - **Full 7-Day Schedule** - Complete timeline
- Timeline visualization
- Historical performance data

### 6. ðŸ“Š Analytics Dashboard
- Comprehensive Twitter/Reddit analytics
- Date range and platform filters
- Hashtag performance analysis
- Emoji impact comparison
- Export options: CSV, PDF, Email
- AI-generated insights

### 7. ðŸ”Œ Data Connections
- API credential management
- Connection testing for all services
- Google Sheets, Twitter, Reddit, Slack
- Security best practices
- Status dashboard

---

## ðŸš€ Quick Start

### 1ï¸âƒ£ Install & Run Locally
```bash
pip install -r requirements.txt
streamlit run Home.py
```
Visit: http://localhost:8501

### 2ï¸âƒ£ Deploy FREE on Streamlit Cloud
1. Push to GitHub
2. Go to https://streamlit.io/cloud
3. Click "New app" â†’ Select repo â†’ Main file: `Home.py`
4. Deploy! ðŸŽ‰

**Full guide:** [`STREAMLIT_DEPLOYMENT.md`](STREAMLIT_DEPLOYMENT.md)

---

## ðŸ’° Total Cost: $0.00 (FREE)

| Service | Cost |
|---------|------|
| Streamlit Cloud | FREE |
| Ollama AI | FREE (Local) |
| Google Sheets | FREE |
| Slack Webhooks | FREE |

---

## ðŸ“ Current Project Structure (Web App)

```
social_data_project/
â”œâ”€â”€ Home.py                           # ðŸŽ¯ Main entry point
â”œâ”€â”€ pages/                            # ðŸ“± Multi-page app
â”‚   â”œâ”€â”€ 1_ðŸ”®_Content_Generator.py
â”‚   â”œâ”€â”€ 2_ðŸ§ª_A_B_Testing_Lab.py
â”‚   â”œâ”€â”€ 3_ðŸ¤–_Engagement_Coach.py
â”‚   â”œâ”€â”€ 4_ðŸ“‹_Campaign_Simulator.py
â”‚   â”œâ”€â”€ 5_ðŸ“Š_Analytics_Dashboard.py
â”‚   â””â”€â”€ 6_ðŸ”Œ_Data_Connections.py
â”œâ”€â”€ utils/                            # ðŸ”§ Backend
â”‚   â”œâ”€â”€ backend.py                    # Cached functions
â”‚   â””â”€â”€ styles.py                     # Custom CSS
â”œâ”€â”€ .streamlit/
â”‚   â””â”€â”€ secrets.toml.template         # Credentials template
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ STREAMLIT_DEPLOYMENT.md           # ðŸ“– Full deployment guide
â”œâ”€â”€ QUICK_START.md                    # âš¡ Quick reference
â””â”€â”€ IMPLEMENTATION_SUMMARY.md         # âœ… Complete summary
```

## ðŸ“ Legacy Backend Files (Data Collection)
social_data_project/
â”‚
â”œâ”€â”€ service_account.json          â† Google API credentials
â”œâ”€â”€ twitter_accounts.json         â† Multi-account Twitter configuration (14 accounts)
â”œâ”€â”€ requirements.txt              â† Python dependencies
â”œâ”€â”€ historical_stats.json         â† Historical engagement patterns (generated)
â”‚
â”œâ”€â”€ google_sheet_connect.py       â† Google Sheets connection
â”œâ”€â”€ twitter_data.py               â† Twitter data fetching
â”œâ”€â”€ twitter_multi_account.py      â† Multi-account rotation system
â”œâ”€â”€ reddit_data.py                â† Reddit data fetching
â”œâ”€â”€ trends_data.py                â† Google Trends data fetching
â”œâ”€â”€ slack_notify.py               â† Slack notification system
â”œâ”€â”€ write_to_sheet.py             â† Write data to sheets
â”œâ”€â”€ main.py                       â† Main data collection script
â”‚
â”œâ”€â”€ data_analysis.py              â† Comprehensive EDA with sentiment & temporal analysis
â”œâ”€â”€ llm_content_generator.py      â† Content generation (saves to sheets + Slack)
â”œâ”€â”€ content_optimizer.py          â† Content optimization (auto & manual params + sheets + Slack)
â”‚
â”œâ”€â”€ ab_testing_tracker.py         â† A/B testing with variant tracking & winner identification (Milestone 3)
â”œâ”€â”€ performance_dashboard.py      â† Real-time metrics dashboard with HTML generation (Milestone 3 & 4)
â”œâ”€â”€ automated_reports.py          â† Weekly/monthly automated reporting system (Milestone 3)
â”œâ”€â”€ advanced_sentiment_analysis.py â† Advanced sentiment analysis with recommendations (Milestone 3)
â”‚
â”œâ”€â”€ engagement_predictor.py       â† Ollama-based engagement prediction (Milestone 4) ðŸ¤– FREE!
â”œâ”€â”€ engagement_predictor_rf_backup.py â† Old Random Forest version (backup)
â”œâ”€â”€ campaign_simulator.py         â† Campaign scenario simulator (Milestone 4) ðŸ”®
â”œâ”€â”€ prediction_coach.py           â† AI-powered campaign advisor (Milestone 4) ðŸŽ¯
â”‚
â”œâ”€â”€ OLLAMA_SETUP_GUIDE.md         â† Complete guide for Ollama setup & usage
â”œâ”€â”€ MILESTONE4_GUIDE.md           â† Milestone 4 documentation
â”œâ”€â”€ README.md                     â† This file
â”‚
â”œâ”€â”€ README.md                     â† This file (setup guide)
â”œâ”€â”€ PROJECT_SUMMARY.md            â† Technical overview & architecture
â”œâ”€â”€ MILESTONE3_GUIDE.md           â† Milestone 3 features documentation
â”œâ”€â”€ MILESTONE4_GUIDE.md           â† Milestone 4 AI/ML features documentation ðŸ¤–
â”œâ”€â”€ KEY_INSIGHTS.md               â† Data analysis findings & recommendations
â”œâ”€â”€ QUICK_ACTION_CARD.md          â† Quick reference for content strategy
â”œâ”€â”€ SLACK_INTEGRATION.md          â† Slack setup guide
â”œâ”€â”€ REDDIT_SETUP.md               â† Reddit API setup guide
â”œâ”€â”€ TWITTER_MULTI_ACCOUNT_SETUP.md â† Multi-account Twitter setup
â””â”€â”€ OLLAMA_SETUP.md               â† LLM setup guide (Ollama installation)
```

## ðŸš€ Setup Instructions

### 1. Prerequisites

- Python 3.8 or higher (Python 3.13.7 tested)
- Google Cloud Platform account
- Twitter Developer account (14 accounts recommended for multi-account rotation)
- Reddit account (for Reddit API)
- Slack workspace (optional, for notifications)
- No API key needed for Google Trends!

### 2. Install Dependencies

```bash
pip install -r requirements.txt
```

### 3. Google Sheets Setup

1. Go to [Google Cloud Console](https://console.cloud.google.com/)
2. Create a new project or select an existing one
3. Enable **Google Sheets API** and **Google Drive API**
4. Create credentials:
   - Go to "Credentials" â†’ "Create Credentials" â†’ "Service Account"
   - Fill in the details and create
   - Click on the created service account
   - Go to "Keys" â†’ "Add Key" â†’ "Create New Key" â†’ JSON
   - Download the JSON file and rename it to `service_account.json`
   - Place it in the project root directory

5. Create a Google Sheet:
   - Go to [Google Sheets](https://sheets.google.com/)
   - Create a new spreadsheet
   - Copy the spreadsheet ID from the URL (the long string between `/d/` and `/edit`)
   - Share the sheet with the service account email (found in `service_account.json`)

### 4. Twitter API Setup

**ðŸ“– See detailed guide: `TWITTER_MULTI_ACCOUNT_SETUP.md`**

**Quick steps:**
1. Go to [Twitter Developer Portal](https://developer.twitter.com/)
2. Create multiple Twitter developer accounts (14 recommended)
3. For each account, get:
   - API Key
   - API Secret Key
   - Bearer Token
   - Access Token
   - Access Token Secret
4. Configure `twitter_accounts.json` with all accounts
5. Add credentials to `.env` for primary account

**Benefits of multi-account:**
- Avoids rate limits (14 accounts = 14x capacity)
- Automatic rotation when limits hit
- Collect more data faster

### 5. Reddit API Setup

**ðŸ“– See detailed guide: `REDDIT_SETUP.md`**

**Quick steps:**
1. Go to [Reddit Apps](https://www.reddit.com/prefs/apps)
2. Create app â†’ Script type
3. Get Client ID and Client Secret
4. Add to `.env`

### 6. Google Trends Setup

**No API key needed!** Just install `pytrends`:
```powershell
pip install pytrends
```

### 7. Slack Notifications (Optional)

**ðŸ“– See detailed guide: `SLACK_INTEGRATION.md`**

**Quick steps:**
1. Create a Slack workspace or use existing
2. Add Incoming Webhooks app
3. Get webhook URL
4. Add to `.env`

### 8. Configure Environment Variables

Edit the `.env` file and add your API keys:

```env
# Twitter/X API credentials (primary account)
TWITTER_API_KEY=your_twitter_api_key
TWITTER_API_SECRET=your_twitter_api_secret
TWITTER_ACCESS_TOKEN=your_access_token
TWITTER_ACCESS_SECRET=your_access_secret
TWITTER_BEARER_TOKEN=your_bearer_token

# Reddit API credentials
REDDIT_CLIENT_ID=your_reddit_client_id
REDDIT_CLIENT_SECRET=your_reddit_client_secret
REDDIT_USER_AGENT=python:social_data_collector:v1.0

# Google Sheets
GOOGLE_SHEET_ID=your_google_sheet_id

# Slack Webhook (optional)
SLACK_WEBHOOK_URL=your_slack_webhook_url
```

### 9. Security - Add `.gitignore`

Create a `.gitignore` file to prevent sensitive files from being committed:

```
# Environment variables
.env

# Google credentials
service_account.json

# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
venv/
env/
ENV/

# IDE
.vscode/
.idea/
*.swp
*.swo
```

## ðŸŽ¯ Usage

### Complete Workflow (3 Steps)

#### Step 1: Collect Data
```bash
python main.py
```

This will:
1. Connect to Google Sheets
2. Fetch data from Google Trends (discover trending topics)
3. Fetch Twitter data using multi-account rotation (2,517 tweets collected)
4. Fetch Reddit data from multiple subreddits (10,070 posts collected)
5. Write all data to separate worksheets
6. Send Slack notifications with progress updates

**Result:** 12,587+ data points collected automatically!

#### Step 2: Analyze Data
```bash
python data_analysis.py
```

This will perform **100% complete EDA**:
- Missing value analysis
- Outlier detection (IQR method)
- Text cleaning and preprocessing
- Statistical summaries (min, max, median, mean, std)
- Correlation analysis
- **Sentiment analysis** (polarity scoring, classification)
- **Temporal trends** (best posting hours/days)
- Platform comparison (Reddit vs Twitter ROI)
- Optimal content length analysis

**Result:** Discover insights like "6 AM Friday = 10x better engagement"!

#### Step 3: Generate Content

**First time? Install Ollama:**
```bash
# Visit: https://ollama.ai/download
# Download and install, then run:
ollama pull llama3.2
ollama serve
```

**Generate AI-powered content:**
```bash
python llm_content_generator.py
```

This will:
- Generate AI-powered posts using Llama 3.2 LLM
- Use real hashtags from your collected data
- Save to Google Sheets â†’ 'Generated_Content' worksheet
- Send Slack notification
- 100% free, runs locally, no API costs

**Optimize content with automatic & manual parameters:**
```bash
python content_optimizer.py
```

This will:
- Analyze content quality (0-100 score)
- Optimize with **automatic parameters** (AI-optimized: temp=0.4, top_p=0.9)
- Optimize with **manual parameters** (interactive user input)
- Compare both approaches
- Save to Google Sheets â†’ 'Optimized_Content' worksheet
- Send Slack notifications for both optimizations

**See setup guide:** `OLLAMA_SETUP.md`

**Result:** High-quality AI-generated and optimized content with parameter comparison!

### Test Individual Components

**Google Sheets Connection:**
```bash
python google_sheet_connect.py
```

**Multi-Account Twitter System:**
```bash
python twitter_multi_account.py
```

**Reddit Integration:**
```bash
python reddit_data.py
```

**Google Trends:**
```bash
python trends_data.py
```

**Slack Notifications:**
```bash
python slack_notify.py
```

## ðŸ“Š Data Collected & Insights

### Data Volume
- **Total:** 12,587 data points
- **Twitter:** 2,517 tweets (using 14-account rotation)
- **Reddit:** 10,070 posts (from multiple subreddits)
- **Google Trends:** Multiple regions analyzed for topic discovery

### Twitter Data Fields
- Tweet ID, created timestamp, text
- Author information (username, name, verified status)
- Engagement metrics (likes, retweets, replies)
- Language, source

### Reddit Data Fields
- Post ID, title, selftext, subreddit
- Author, score, upvote ratio
- Number of comments
- Created timestamp, URL

### Google Trends Data
- Trending searches by region (US, UK, India)
- Used for topic discovery to search Twitter/Reddit

### Key Insights Discovered

ðŸ“Š **Platform Comparison:**
- Reddit provides **2,053x better engagement** than Twitter
- Reddit avg: 5,172 engagement per post
- Twitter avg: 2.4 engagement per tweet

â° **Best Posting Times (Twitter):**
- **6:00 AM on Friday** = 10x better engagement
- Weekdays are +86% better than weekends
- Evening backup: 8:00 PM

ðŸ“ **Optimal Content Length:**
- Reddit titles: 50-100 characters = +22% engagement
- Twitter: ~234 characters optimal

ðŸ˜ **Sentiment Impact:**
- 51.5% of content is neutral (informational)
- Sentiment has minimal impact (-0.030 correlation)
- **Key takeaway:** Focus on information quality over emotional tone

#ï¸âƒ£ **Top Hashtags:**
- #DigitalMarketing (77 uses)
- #SEO (54 uses)
- #AI (26 uses)

ðŸ”‘ **Top Keywords:**
- marketing (2,788 mentions)
- market (2,134 mentions)
- media (1,093 mentions)

**ðŸ“– See full analysis:** `KEY_INSIGHTS.md`  
**ðŸ“‹ Quick reference:** `QUICK_ACTION_CARD.md`

## ðŸ”§ Customization

### Modify Twitter Search Query

In `main.py`, change the query parameter:

```python
twitter_df = fetch_tweets(
    client,
    query="your custom query -is:retweet",
    max_results=100
)
```

### Multi-Account Twitter Usage

The system automatically rotates through 14 accounts:
```python
# Configured in twitter_accounts.json
twitter_manager = authenticate_twitter_multi()
client = twitter_manager.get_current_client()  # Auto-rotates on rate limit
```

### Change Number of Posts

Adjust `limit` or `max_results` parameters:

```python
# Twitter (per account)
twitter_df = fetch_tweets(client, query="...", max_results=100)

# Reddit
reddit_df = search_reddit(reddit_client, query="AI marketing", limit=100)
```

### Customize Reddit Data Collection

```python
# Search by keyword across Reddit
reddit_df = search_reddit(reddit_client, query="digital marketing", limit=100)

# Fetch from specific subreddit
subreddit_df = fetch_subreddit_posts(reddit_client, "marketing", limit=50)
```

### Customize Google Trends

```python
# Different regions
trends_df = get_trending_searches(pytrends, country='india')

# Multiple regions for more topics
trends_us = get_trending_searches(pytrends, country='united_states')
trends_uk = get_trending_searches(pytrends, country='united_kingdom')
```

### Analyze Different Datasets

Edit `data_analysis.py` to analyze specific worksheets:
```python
# Analyze only Twitter data
twitter_df = worksheet.get_all_records()
analyze_platform_data(twitter_df, "Twitter")

# Analyze only Reddit data
reddit_df = worksheet.get_all_records()
analyze_platform_data(reddit_df, "Reddit")
```

## ðŸ“ Notes

- **Rate Limits**: 
  - Twitter: Multi-account system handles limits automatically (14 accounts = 14x capacity)
  - Reddit: 60 requests/minute (code handles with delays)
  - Google Trends: Rate limited (delays added between requests)
  - Slack: No strict limits for webhooks
  
- **Data Volume**: 
  - Currently: 12,587 data points collected
  - Twitter: 2,517 tweets across 14 topics
  - Reddit: 10,070 posts from multiple subreddits
  
- **Service Account**: Make sure Google Sheet is shared with service account email

- **Data Privacy**: Never commit `.env`, `service_account.json`, or `twitter_accounts.json` to Git

- **Best Practices**:
  - Multi-account rotation prevents rate limit issues
  - Run data analysis after collection completes
  - Use insights from KEY_INSIGHTS.md for content strategy
  - Check QUICK_ACTION_CARD.md for quick reference
  - Monitor via Slack notifications

## ðŸ› Troubleshooting

### Google Sheets Issues

**"service_account.json not found"**
- Download credentials from Google Cloud Console
- Place in project root directory

**"Error 403: Permission denied"**
- Share Google Sheet with service account email
- Enable Google Sheets API and Google Drive API

### Twitter Issues

**"Authentication failed"**
- Verify API credentials in `.env`
- Check Twitter Developer account access level
- Ensure Bearer Token is correct

**"Rate limit exceeded"**
- Multi-account system should handle this automatically
- Check `twitter_accounts.json` configuration
- See `TWITTER_MULTI_ACCOUNT_SETUP.md`

### Reddit Issues

**"401 Unauthorized"**
- Check Client ID and Secret in `.env`
- Verify app type is "script"
- See `REDDIT_SETUP.md`

**"429 Rate limit"**
- Reddit limits: 60 requests/minute
- Code handles this with `time.sleep()` delays

### Google Trends Issues

**"429 Too Many Requests"**
- Google Trends has aggressive rate limits
- Code adds delays (`time.sleep(3)`) between requests
- Avoid running trends fetching too frequently

### Slack Notification Issues

**"Webhook URL invalid"**
- Verify URL in `.env` starts with `https://hooks.slack.com/`
- Check webhook is active in Slack workspace
- See `SLACK_INTEGRATION.md`

### Data Analysis Issues

**"TextBlob not found"**
- Install: `pip install textblob`
- Download corpora: `python -m textblob.download_corpora`

**"No module named 'data_analysis'"**
- Ensure you're in the project directory
- Run: `python data_analysis.py` (not as module import)

## ðŸ“š Resources

### Official Documentation
- [Google Sheets API](https://developers.google.com/sheets/api)
- [Twitter API v2](https://developer.twitter.com/en/docs/twitter-api)
- [Reddit API (PRAW)](https://praw.readthedocs.io/)
- [Google Trends (pytrends)](https://pypi.org/project/pytrends/)
- [Slack Incoming Webhooks](https://api.slack.com/messaging/webhooks)
- [TextBlob Documentation](https://textblob.readthedocs.io/)

### Python Libraries
- [Tweepy](https://docs.tweepy.org/) - Twitter API wrapper
- [gspread](https://docs.gspread.org/) - Google Sheets API wrapper
- [PRAW](https://praw.readthedocs.io/) - Reddit API wrapper
- [pandas](https://pandas.pydata.org/) - Data manipulation
- [TextBlob](https://textblob.readthedocs.io/) - Sentiment analysis

### Project Documentation
- `PROJECT_SUMMARY.md` - Technical overview & architecture
- `KEY_INSIGHTS.md` - Data analysis findings & strategic recommendations â­
- `QUICK_ACTION_CARD.md` - Quick reference for content strategy â­
- `TWITTER_MULTI_ACCOUNT_SETUP.md` - Multi-account Twitter setup
- `REDDIT_SETUP.md` - Reddit API setup guide
- `SLACK_INTEGRATION.md` - Slack webhook configuration

### Key Insights Documents
ðŸ“Š **Must Read:**
- `KEY_INSIGHTS.md` - Discover why Reddit is 2,053x better, optimal posting times, and more
- `QUICK_ACTION_CARD.md` - Print this! Quick reference for daily content decisions

## ðŸ“§ Support

For issues or questions:
1. Check the relevant setup guide (TWITTER_MULTI_ACCOUNT_SETUP.md, REDDIT_SETUP.md, etc.)
2. Review troubleshooting section above
3. Check KEY_INSIGHTS.md for data-related questions
4. Review PROJECT_SUMMARY.md for technical architecture

## ðŸŽ¯ Quick Start Summary

**Complete workflow (11 commands):**

```bash
# MILESTONE 1 & 2: Data Collection & Generation
# 1. Collect data (12,587+ data points)
python main.py

# 2. Analyze data (100% EDA with insights)
python data_analysis.py

# 3. Generate content with AI (based on real insights)
# First time: ollama pull llama3.2 && ollama serve
python llm_content_generator.py

# 4. Optimize content (automatic & manual parameters)
python content_optimizer.py

# MILESTONE 3: Advanced Analytics & Tracking
# 5. Track A/B test variants and identify winners
python ab_testing_tracker.py

# 6. Generate performance dashboard (HTML)
python performance_dashboard.py

# 7. Create automated weekly/monthly reports
python automated_reports.py

# 8. Run advanced sentiment analysis with recommendations
python advanced_sentiment_analysis.py

# MILESTONE 4: AI-Powered Predictions ðŸ¤–
# 9. Train ML model (first time only, then retrain weekly)
python engagement_predictor.py

# 10. Simulate campaign scenarios (timing, content, platform)
python campaign_simulator.py

# 11. Get AI coaching and recommendations
python prediction_coach.py
```

**Then check:**
- **Google Sheets:**
  - `Generated_Content` - All AI-generated posts
  - `Optimized_Content` - Optimized versions with parameters
  - `Campaign_Simulations` - Simulated campaign results ðŸ¤–
  - `Coaching_Sessions` - AI recommendations ðŸŽ¯
- **Slack:** Real-time notifications for generation, optimization & predictions
- **HTML Dashboard:** `performance_dashboard.html` with ML model status
- `KEY_INSIGHTS.md` - Full analysis findings
- `QUICK_ACTION_CARD.md` - Quick reference card
- `MILESTONE4_GUIDE.md` - AI/ML features guide ðŸ¤–

**Key Achievements:** ðŸ†
- Reddit is 2,053x better than Twitter
- Post at 6 AM Friday for 10x engagement
- 50-100 char titles = +22% engagement
- Information quality > emotional tone
- **ML model predicts engagement with 78% accuracy** ðŸ¤–
- **Campaign simulator tests 42+ scenarios** ðŸ”®
- **AI coach provides personalized recommendations** ðŸŽ¯
- **NEW:** Automatic & manual LLM parameter optimization
- **NEW:** Dual Google Sheets storage for tracking

---

**Happy Data Analyzing! ðŸ“ŠðŸš€**

**Status:** âœ… Production Ready - Milestones 1 & 2 Complete
